{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def problem1(filename):\n",
    "    # Read exel file \n",
    "    data=pd.read_excel(filename)\n",
    "    # Get features from dropping outputs\n",
    "    X = data.drop(['Y1', 'Y2'], axis=1).values\n",
    "    # Get outputs from data\n",
    "    y_1= ((data['Y1']).values).reshape(-1, 1)\n",
    "    y_2= ((data['Y2']).values).reshape(-1, 1)\n",
    "    # Import libraries\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    # Set paramaters for random forest regressor\n",
    "    param_grid = {\n",
    "        'max_depth': [50,150,250],\n",
    "        'min_samples_leaf': [1,2,3],\n",
    "        'min_samples_split': [2,3],\n",
    "        'n_estimators': [10,50,100,250,500]\n",
    "    }\n",
    "    # Set parameters for ridge regressor\n",
    "    ridge_params={\n",
    "        'alpha':[0.001,0.01,0.1, 1.0, 10.0]\n",
    "    }\n",
    "    # This list is used in loop. \n",
    "    # The outer loop iterate each output y_1 and y_2 and the inner loop iterate two types error\n",
    "    outputs=[y_1,y_2]\n",
    "    ridge_means=[]\n",
    "    ridge_std=[]\n",
    "    forest_means=[]\n",
    "    forest_std=[]\n",
    "    # Outer loop\n",
    "    for index,y_x in  enumerate(outputs):\n",
    "        # Inner loop\n",
    "        for score in [\"neg_mean_absolute_error\",\"neg_mean_squared_error\"]:\n",
    "            # Split data randomly.\n",
    "            X_train , X_test , y_train , y_test = train_test_split(X , y_x , test_size=0.2 , random_state =42)\n",
    "            # Scale data for get good model\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            scaler=StandardScaler()\n",
    "            X_train=scaler.fit_transform(X_train)\n",
    "            X_test=scaler.transform(X_test)\n",
    "            # 10-fold 10-repetative cross validation method \n",
    "            from sklearn.model_selection import RepeatedKFold,cross_val_score\n",
    "            cv=RepeatedKFold(n_splits=10,n_repeats=10,random_state=True)\n",
    "            temp_ridge=Ridge()\n",
    "            # In this line, using gridsearchcv, we can get best parameters for ridge regression\n",
    "            grid_search_ridge=GridSearchCV(estimator = temp_ridge, param_grid = ridge_params, cv = cv, n_jobs = -1)\n",
    "            grid_search_ridge.fit(X_train,y_train.ravel())\n",
    "            print(\"Best parameter for ridge regression\",grid_search_ridge.best_params_)\n",
    "            # Using best parameters, the best model is created.\n",
    "            model=Ridge(**grid_search_ridge.best_params_)\n",
    "            model.fit(X_train,y_train)\n",
    "            # Using best model, we can get mean and standart deviation\n",
    "            scores_ridge = cross_val_score(model, X_train, y_train,scoring=score, cv=cv)\n",
    "            ridge_means.append(-scores_ridge.mean())\n",
    "            ridge_std.append(scores_ridge.std())\n",
    "            # Create a random forest regressor\n",
    "            rf = RandomForestRegressor()\n",
    "            # Find best parameters using grid search\n",
    "            grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = cv, n_jobs = -1)\n",
    "            grid_search.fit(X_train,y_train.ravel())\n",
    "            print(\"Best parameter for Random forest regression\",grid_search.best_params_)\n",
    "            # create a model using best parameters\n",
    "            last_model=RandomForestRegressor(**grid_search.best_params_)\n",
    "            scores_randomforest= cross_val_score(last_model, X_train, y_train.ravel(),scoring=score, cv=cv)\n",
    "            forest_means.append(-scores_randomforest.mean())\n",
    "            forest_std.append(scores_randomforest.std())\n",
    "            # print(\"RandomForest\",\"Y\",index+1,\" \",score,\"-----\",-scores.mean(),scores.std()\n",
    "    # In this line prettytable is used to print results as table.\n",
    "    from prettytable import PrettyTable\n",
    "    x=PrettyTable([\"-\",\"Mean Absolute Error\",\"*\",\"Mean Square Error\",\"/\"])\n",
    "    x.add_row([\"Output\",\"RandomForest\",\"RidgeRegression\",\"RandomForest\",\"RidgeRegression\"])\n",
    "    x.add_row([\"Y1\",str(forest_means[0])+u\"\\u00B1\"+str(forest_std[0]),str(ridge_means[0])+u\"\\u00B1\"+str(ridge_std[0]),\n",
    "                    str(forest_means[1])+u\"\\u00B1\"+str(forest_std[1]),str(ridge_means[1])+u\"\\u00B1\"+str(ridge_std[1])])\n",
    "    x.add_row([\"Y2\",str(forest_means[2])+u\"\\u00B1\"+str(forest_std[2]),str(ridge_means[2])+u\"\\u00B1\"+str(ridge_std[2]),\n",
    "                    str(forest_means[3])+u\"\\u00B1\"+str(forest_std[3]),str(ridge_means[3])+u\"\\u00B1\"+str(ridge_std[3])])\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter for ridge regression {'alpha': 0.1}\n",
      "Best parameter for Random forest regression {'max_depth': 50, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500}\n",
      "Best parameter for ridge regression {'alpha': 0.1}\n",
      "Best parameter for Random forest regression {'max_depth': 250, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500}\n",
      "Best parameter for ridge regression {'alpha': 0.1}\n",
      "Best parameter for Random forest regression {'max_depth': 50, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500}\n",
      "Best parameter for ridge regression {'alpha': 0.1}\n",
      "Best parameter for Random forest regression {'max_depth': 150, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500}\n",
      "+--------+---------------------------------------+----------------------------------------+-----------------------------------------+--------------------------------------+\n",
      "|   -    |          Mean Absolute Error          |                   *                    |            Mean Square Error            |                  /                   |\n",
      "+--------+---------------------------------------+----------------------------------------+-----------------------------------------+--------------------------------------+\n",
      "| Output |              RandomForest             |            RidgeRegression             |               RandomForest              |           RidgeRegression            |\n",
      "|   Y1   | 0.3269955335096064±0.0479998565883446 | 2.0684062874150566±0.24478405555575394 | 0.24826960552234414±0.10061971286789874 | 8.589288373986665±1.791267376161846  |\n",
      "|   Y2   |  0.999799514542574±0.1582350909855546 | 2.2830741086533677±0.2724593537075571  |  2.7957592061379235±0.8485947796206003  | 10.481908726214302±2.523616365288306 |\n",
      "+--------+---------------------------------------+----------------------------------------+-----------------------------------------+--------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "problem1(\"ENB2012_data.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "from sklearn.model_selection import train_test_split,RepeatedKFold,KFold,cross_val_score,GridSearchCV,StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "def problem2(filename):\n",
    "    # Read data using pandas library\n",
    "    data = pd.read_csv(filename, sep = ';')\n",
    "    #Converting dependent variable categorical to dummy\n",
    "    y = pd.get_dummies(data['y'], columns = ['y'], prefix = ['y'], drop_first = True)\n",
    "    y=y.values.reshape(-1, 1)\n",
    "    # Firstly I divided categorical data different parts. \n",
    "    # data_c includes client features\n",
    "    # I used labelencoder because it gives better result\n",
    "    data_c = data.iloc[: , 0:7]\n",
    "    labelencoder_X = LabelEncoder()\n",
    "    data_c['job']      = labelencoder_X.fit_transform(data_c['job']) \n",
    "    data_c['marital']  = labelencoder_X.fit_transform(data_c['marital']) \n",
    "    data_c['education']= labelencoder_X.fit_transform(data_c['education']) \n",
    "    data_c['default']  = labelencoder_X.fit_transform(data_c['default']) \n",
    "    data_c['housing']  = labelencoder_X.fit_transform(data_c['housing']) \n",
    "    data_c['loan']     = labelencoder_X.fit_transform(data_c['loan']) \n",
    "    # The client age is divided four range. I get the idea different projects. It is useful\n",
    "    data_c.loc[data_c['age'] <= 32, 'age'] = 1\n",
    "    data_c.loc[(data_c['age'] > 32) & (data_c['age'] <= 47), 'age'] = 2\n",
    "    data_c.loc[(data_c['age'] > 47) & (data_c['age'] <= 70), 'age'] = 3\n",
    "    data_c.loc[(data_c['age'] > 70) & (data_c['age'] <= 98), 'age'] = 4\n",
    "    # try to convert contact, month, day_of_week data to numerical\n",
    "    data_r = data.iloc[: , 7:11]\n",
    "    data[(data['duration'] == 0)]\n",
    "    labelencoder_X = LabelEncoder()\n",
    "    data_r['contact']     = labelencoder_X.fit_transform(data_r['contact']) \n",
    "    data_r['month']       = labelencoder_X.fit_transform(data_r['month']) \n",
    "    data_r['day_of_week'] = labelencoder_X.fit_transform(data_r['day_of_week']) \n",
    "    # I did samething like before  I did to age column\n",
    "    data_r.loc[data['duration'] <= 102, 'duration'] = 1\n",
    "    data_r.loc[(data['duration'] > 102) & (data_r['duration'] <= 180)  , 'duration']    = 2\n",
    "    data_r.loc[(data['duration'] > 180) & (data_r['duration'] <= 319)  , 'duration']   = 3\n",
    "    data_r.loc[(data['duration'] > 319) & (data_r['duration'] <= 644.5), 'duration'] = 4\n",
    "    data_r.loc[data['duration']  > 644.5, 'duration'] = 5\n",
    "\n",
    "    data_s = data.loc[: , ['emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']]\n",
    "    data_o = data.loc[: , ['campaign', 'pdays','previous', 'poutcome']]\n",
    "    data_o['poutcome'].replace(['nonexistent', 'failure', 'success'], [1,2,3], inplace  = True)\n",
    "    # To combine data after data preprocessing\n",
    "    data_final= pd.concat([data_c, data_r, data_s, data_o], axis = 1)\n",
    "    data_final = data_final[['age', 'job', 'marital', 'education', 'default', 'housing', 'loan',\n",
    "                        'contact', 'month', 'day_of_week', 'duration', 'emp.var.rate', 'cons.price.idx', \n",
    "                        'cons.conf.idx', 'euribor3m', 'nr.employed', 'campaign', 'pdays', 'previous', 'poutcome']]\n",
    "    # Split data \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_final, y.ravel(), test_size = 0.2, random_state = 101)\n",
    "    # Scale data\n",
    "    sc_X = StandardScaler()\n",
    "    X_train = sc_X.fit_transform(X_train)\n",
    "    X_test = sc_X.transform(X_test)\n",
    "    # create cross validation method\n",
    "    cv=RepeatedKFold(n_splits=5,n_repeats=5,random_state=True)\n",
    "    # Create a loop that is range from 10^-4 to 10^4\n",
    "    # And put each values as C parameter\n",
    "    all_scores=[]\n",
    "    for x in np.logspace(-4,4,20):\n",
    "        model=LogisticRegression(C=x)\n",
    "        model.fit(X_train,y_train)\n",
    "        scores=cross_val_score(model, X_train, y_train.ravel(),scoring=\"roc_auc\", cv=cv)\n",
    "        all_scores.append(scores.mean())\n",
    "\n",
    "    # Plot scores\n",
    "    plt.plot(np.logspace(-4,4,20,endpoint=True),all_scores,'-gD')\n",
    "    plt.xscale(\"log\")\n",
    "    plt.ylabel(\"Mean Auc Score\")\n",
    "    plt.title(\"Logistic Regression Model\")\n",
    "    # Create paramater list \n",
    "    param_grid = {\n",
    "    'max_depth': [50,150,250],\n",
    "    'min_samples_leaf': [1,2,3],\n",
    "    'min_samples_split': [2,3],\n",
    "    'n_estimators': [10,50,100,250,500,1000]\n",
    "    }\n",
    "    # Create cross validation method\n",
    "    cv=RepeatedKFold(n_splits=3,n_repeats=3,random_state=True)\n",
    "    rf = RandomForestRegressor()\n",
    "    grid_search_logi = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                            cv = cv, n_jobs = -1)\n",
    "    \n",
    "    grid_search_logi.fit(X_train,y_train.ravel())\n",
    "    # get best parameters for random forest regressor\n",
    "    print(\"Best parameters of Random regressor\",grid_search_logi.best_params_);\n",
    "    last_model_logi=RandomForestRegressor(**grid_search_logi.best_params_)\n",
    "    scores = cross_val_score(last_model_logi, X_train, y_train.ravel(),scoring=\"roc_auc\", cv=cv)\n",
    "    scores.mean()\n",
    "    # create neural network using MLPClassifier\n",
    "    mlp_classifier=MLPClassifier(max_iter=500)\n",
    "    # Create parameter list\n",
    "    parameter_space = {\n",
    "        'hidden_layer_sizes': [(10,10,10),(10,10,10,10),(10,10,10,10,10),(10,10,10,10,10,10)],\n",
    "        'alpha': [0.00001, 0.0001,0.001, 0.01, 0.1],\n",
    "    }\n",
    "    grid_search = GridSearchCV(estimator = mlp_classifier, param_grid = parameter_space, \n",
    "                            cv = cv, n_jobs = -1,scoring='roc_auc')\n",
    "    grid_search.fit(X_train,y_train.ravel())\n",
    "    print(\"Best parameters for Neural Network\",grid_search.best_params_)\n",
    "    # Variables for average classification report\n",
    "    from sklearn.metrics import classification_report,make_scorer,accuracy_score\n",
    "    originalclass = []\n",
    "    predictedclass = []\n",
    "    #Make our customer score\n",
    "    def classification_report_with_accuracy_score(y_true, y_pred):\n",
    "        originalclass.extend(y_true)\n",
    "        predictedclass.extend(y_pred)\n",
    "        return accuracy_score(y_true, y_pred) # return accuracy score\n",
    "\n",
    "\n",
    "    logistic_reg=LogisticRegression(C=1)\n",
    "    best_neural=MLPClassifier(**grid_search.best_params_)\n",
    "    best_randomforest=RandomForestClassifier(**grid_search_logi.best_params_)\n",
    "    # best_randomforest.fit(X_train,y_train.ravel())\n",
    "    # y_pred=best_randomforest.predict(X_test)\n",
    "    outer_cv = KFold(n_splits=5, shuffle=True, random_state=True)\n",
    "    # Nested CV with parameter optimization\n",
    "    nested_score = cross_val_score(logistic_reg, X=X_train, y=y_train.ravel(), cv=outer_cv, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "    print(\"classification report for logistic regression\")\n",
    "    print(classification_report(originalclass, predictedclass)) \n",
    "    originalclass.clear()\n",
    "    predictedclass.clear()\n",
    "    nested_score = cross_val_score(best_neural, X=X_train, y=y_train.ravel(), cv=outer_cv, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "    print(\"classification report for neural network\")\n",
    "    print(classification_report(originalclass, predictedclass)) \n",
    "    originalclass.clear()\n",
    "    predictedclass.clear()\n",
    "    nested_score = cross_val_score(best_randomforest, X=X_train, y=y_train.ravel(), cv=outer_cv, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "    print(\"classification report for Random forest\")\n",
    "    print(classification_report(originalclass, predictedclass)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters of Random regressor {'max_depth': 150, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "Best parameters for Neural Network {'alpha': 0.1, 'hidden_layer_sizes': (10, 10, 10, 10, 10)}\n",
      "classification report for logistic regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95     29269\n",
      "           1       0.67      0.38      0.49      3681\n",
      "\n",
      "    accuracy                           0.91     32950\n",
      "   macro avg       0.80      0.68      0.72     32950\n",
      "weighted avg       0.90      0.91      0.90     32950\n",
      "\n",
      "classification report for neural network\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95     29269\n",
      "           1       0.62      0.55      0.58      3681\n",
      "\n",
      "    accuracy                           0.91     32950\n",
      "   macro avg       0.78      0.75      0.77     32950\n",
      "weighted avg       0.91      0.91      0.91     32950\n",
      "\n",
      "classification report for Random forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95     29269\n",
      "           1       0.65      0.48      0.55      3681\n",
      "\n",
      "    accuracy                           0.91     32950\n",
      "   macro avg       0.79      0.72      0.75     32950\n",
      "weighted avg       0.90      0.91      0.91     32950\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "problem2('bank-additional-full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11f4f9e024f345a762b367b953dda8a9d0d73d0ed685f40724d283ac032facc1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('sklearn-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
